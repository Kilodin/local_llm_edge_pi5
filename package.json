{
  "name": "local-llm-inference",
  "version": "1.0.0",
  "description": "Local LLM inference using llama.cpp with C++ core and Node.js bindings",
  "main": "src/server/index.js",
  "scripts": {
    "install": "node-gyp rebuild",
    "build": "cmake -B build && cmake --build build",
    "build:cpp": "cmake -B build && cmake --build build",
    "build:ui": "cd src/ui && npm run build",
    "dev:server": "LD_LIBRARY_PATH=$(pwd)/third_party/llama.cpp/build/bin:$(pwd)/build/Release nodemon src/server/index.js",
    "dev:ui": "cd src/ui && npm start",
    "dev": "concurrently \"npm run dev:server\" \"npm run dev:ui\"",
    "start": "LD_LIBRARY_PATH=$(pwd)/third_party/llama.cpp/build/bin:$(pwd)/build/Release node src/server/index.js",
    "start:helper": "node server-helper.js",
    "start:with-helper": "concurrently \"npm run start:helper\" \"npm run start\"",
    "test": "LD_LIBRARY_PATH=$(pwd)/third_party/llama.cpp/build/bin:$(pwd)/build/Release node test/test.js",
    "cli": "LD_LIBRARY_PATH=$(pwd)/third_party/llama.cpp/build/bin:$(pwd)/build/Release node src/cli/index.js"
  },
  "keywords": ["llm", "llama", "inference", "local", "cpp", "nodejs", "react"],
  "author": "Your Name",
  "license": "MIT",
  "dependencies": {
    "node-addon-api": "^7.0.0",
    "express": "^4.18.2",
    "cors": "^2.8.5",
    "socket.io": "^4.7.2",
    "commander": "^11.0.0",
    "chalk": "^5.3.0",
    "multer": "^2.0.0-rc.3",
    "fs-extra": "^11.1.1"
  },
  "devDependencies": {
    "node-gyp": "^10.0.0",
    "nodemon": "^3.0.1",
    "concurrently": "^8.2.0"
  },
  "gypfile": true
} 