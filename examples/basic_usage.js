const { LLMNodeBinding } = require('../build/Release/llm_node');

async function basicExample() {
    console.log('ğŸ¤– Local LLM Inference - Basic Example\n');
    
    // Create LLM instance
    const llm = new LLMNodeBinding();
    
    // Show system information
    console.log('ğŸ“Š System Information:');
    console.log(LLMNodeBinding.getSystemInfo());
    console.log();
    
    // Initialize model
    console.log('ğŸ”§ Initializing model...');
    const success = llm.initialize({
        modelPath: './models/tinyllama-1.1b-chat.gguf',
        contextSize: 2048,
        batchSize: 512,
        threads: 4,
        gpuLayers: 0,
        temperature: 0.7,
        topP: 0.9,
        topK: 40,
        repeatPenalty: 1.1,
        seed: 42
    });
    
    if (!success) {
        console.error('âŒ Failed to initialize model');
        return;
    }
    
    console.log('âœ… Model initialized successfully!');
    console.log('\nğŸ“‹ Model Information:');
    console.log(llm.getModelInfo());
    console.log();
    
    // Generate text
    console.log('ğŸ’¬ Generating text...');
    const prompt = "Hello! Can you tell me a short joke?";
    console.log(`ğŸ“ Prompt: ${prompt}`);
    
    const result = llm.generate(prompt, 100);
    console.log(`ğŸ¤– Response: ${result}`);
    console.log();
    
    // Update parameters
    console.log('âš™ï¸ Updating parameters...');
    llm.setTemperature(0.9);
    llm.setTopP(0.8);
    console.log('âœ… Parameters updated');
    console.log();
    
    // Generate with new parameters
    console.log('ğŸ’¬ Generating with new parameters...');
    const newPrompt = "Write a haiku about programming";
    console.log(`ğŸ“ Prompt: ${newPrompt}`);
    
    const newResult = llm.generate(newPrompt, 50);
    console.log(`ğŸ¤– Response: ${newResult}`);
    console.log();
    
    console.log('ğŸ‰ Example completed!');
}

// Run example
if (require.main === module) {
    basicExample().catch(console.error);
}

module.exports = { basicExample }; 